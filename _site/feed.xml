<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matt Giguere</title>
    <description>Hi, I&#39;m Matt Giguere, a graduate student searching for rocky planets and characterizing stellar activity of the stars that host them. In addition to working towards the discovery  of other rocky worlds, I am constantly working to defend my best dad  of the year title. I also enjoy learning new programming languages,  new statistical methods, and new visualization tools, and making  things in many different media.
</description>
    <link>http://mattgiguere.github.io/</link>
    <atom:link href="http://mattgiguere.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 20 Mar 2015 14:56:38 -0400</pubDate>
    <lastBuildDate>Fri, 20 Mar 2015 14:56:38 -0400</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Better Python Coding Practices</title>
        <description>&lt;p&gt;Recently I’ve been interested in improving my coding practices, as
everyone should be. There are a few new tools that I have been
implementing that have already paid off in improving my end product.
In this blog post, I will describe some of these tools and show how
I got them up and running.&lt;/p&gt;

&lt;h3 id=&quot;unit-testing&quot;&gt;Unit Testing&lt;/h3&gt;
&lt;p&gt;The first thing that &lt;strong&gt;everyone&lt;/strong&gt; should do is &lt;a href=&quot;http://en.wikipedia.org/wiki/Unit_testing&quot;&gt;unit testing&lt;/a&gt;.
Unit testing ensures that bits of the code behave the way they are
expected to behave. This is done by writing simple test routines
that analyze the output of the code of interest.&lt;/p&gt;

&lt;p&gt;Below is a simple example.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Routine to test:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multiply_numbers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Test routine:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_multiply_numbers_with_scalars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multiply_numbers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This test ensures that the result the &lt;code&gt;multiply_numbers&lt;/code&gt; function
is the length that we would expect it to be. For simple one-liners
 like &lt;code&gt;multiply_numbers&lt;/code&gt;, there’s no need to test, but functions
 and methods in the real world are almost always more complicated,
 and as the complexity increases testing can save a ton of time
 that would otherwise be spent debugging.&lt;/p&gt;

&lt;p&gt;There are many libraries out there for testing python code, but
&lt;a href=&quot;https://nose.readthedocs.org/en/latest/&quot;&gt;nose&lt;/a&gt; looked the most appealing to me. How to set nose up
is detailed in their documentation. Personally, I used &lt;code&gt;pip&lt;/code&gt; to
install it:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;pip install nose&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I then created a “tests” directory in my repository root directory
and started filling it with short test routines to test out every
function and possibility of my code. Then to test your code with
nose, simply type &lt;code&gt;nosetests&lt;/code&gt; at the command line at the repository
root level. nose searches through your repository looking for test
code and executes the tests it finds. Here’s an example of nose
successfully executing all 12 of test routines it found in my
repository path:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;my_project&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;nosetests
............
----------------------------------------------------------------------
Ran &lt;span class=&quot;m&quot;&gt;12&lt;/span&gt; tests in 0.014s

OK
my_project&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you’re curious what other testing packages exist out there for
python, I found the
&lt;a href=&quot;http://docs.python-guide.org/en/latest/writing/tests/&quot;&gt;Hitchhiker’s Guide to Python&lt;/a&gt; to give an excellent
introduction to unit testing in python.&lt;/p&gt;

&lt;h3 id=&quot;travis-ci&quot;&gt;Travis-CI&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://travis-ci.org&quot;&gt;Travis-CI&lt;/a&gt; is a continuous integration service. This allows for
automated testing of code upon pushes to a &lt;a href=&quot;https://github.com&quot;&gt;github&lt;/a&gt; repository.
This free tool for open source projects is great if you forget to run
&lt;code&gt;nosetests&lt;/code&gt; after adding new functionality to your code and also if
your collaborating with others. It will probably take a little time
to get travis setup for your particular needs. The Travis-CI
&lt;a href=&quot;http://docs.travis-ci.com/user/languages/python/&quot;&gt;docs&lt;/a&gt; are a good place to start, but I found their built
in python versions did not satisfy all of the dependencies for just
about any of my projects. Instead of building python and the
dependencies from scratch (e.g. numpy, scipy, pandas, etc),
continuum.io has a terrific solution with miniconda. I found
&lt;a href=&quot;http://conda.pydata.org/docs/travis.html&quot;&gt;this documentation&lt;/a&gt; to be quite helpful in setting up
travis-ci to work with my code. Once you get it working Travis-CI
provides a pretty badge that you can put in your repository README
to tell the world that your code has been tested, and (hopefully)
it passed!&lt;/p&gt;

&lt;p&gt;Here’s an example from one of my repositories of what the badge looks
like.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://travis-ci.org/mattgiguere/pyutil&quot;&gt;&lt;img src=&quot;https://travis-ci.org/mattgiguere/pyutil.svg?branch=master&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you push a commit to github that doesn’t pass testing, it turns
red and says “failed”. Don’t worry, you’ll also receive an email
notification informing you of the change in status so you can
quickly repair the damages.&lt;/p&gt;

&lt;h3 id=&quot;coveralls&quot;&gt;Coveralls&lt;/h3&gt;

&lt;p&gt;Now you have testing setup, and you have travis-ci setup to
perform continuous integration and make sure your latest commit
is performing the way you expect it to! But &lt;em&gt;how much&lt;/em&gt; of the
code is actually being tested? Travis-CI will tell you if all
of the tests you made ran successfully, but what if you are only
testing a tiny fraction of one of your routines? That’s where
&lt;a href=&quot;https://coveralls.io&quot;&gt;coveralls&lt;/a&gt; comes in to play. Coveralls automatically
analyzes your tests and code to see what fraction of your code
is actually being covered by your tests. Not only does it check
to see if all of your subroutines and methods are being tested,
but if actually looks into all of the conditional cases to make
sure all of the exceptional cases are being handled (i.e., it
looks to make sure you’re testing all of the
&lt;a href=&quot;http://en.wikipedia.org/wiki/Edge_case&quot;&gt;edge cases&lt;/a&gt;). Once you get coveralls up and running,
not only will it tell you what percent of your code you are
actually testing, but through its site you can zoom in and easily
see what parts of your code you are missing with your tests. This is
a great tool for checking to make sure you are covering all
cases with your tests.&lt;/p&gt;

&lt;p&gt;Coveralls can be integrated with travis to run once travis finishes
with its tests. Like travis, coveralls comes with another pretty
badge to show the world what percent of your code has been tested.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://coveralls.io/r/mattgiguere/pyutil?branch=master&quot;&gt;&lt;img src=&quot;https://coveralls.io/repos/mattgiguere/pyutil/badge.svg?branch=master&quot; alt=&quot;Coverage Status&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;codacy&quot;&gt;Codacy&lt;/h3&gt;

&lt;p&gt;Lastly, there’s &lt;a href=&quot;https://www.codacy.com/login&quot;&gt;codacy&lt;/a&gt;, another great tool that is
free for open source software projects. Like coveralls and
travis, codacy works with github repositories (and many others)
and looks for commits to your repository. Upon new commits, it
regrades your coding and provides a report card on how your
code stacks up in several categories. These categories are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Code Complexity&lt;/li&gt;
  &lt;li&gt;Code Style&lt;/li&gt;
  &lt;li&gt;Compatibility&lt;/li&gt;
  &lt;li&gt;Documentation&lt;/li&gt;
  &lt;li&gt;Error Prone&lt;/li&gt;
  &lt;li&gt;Performance&lt;/li&gt;
  &lt;li&gt;Security&lt;/li&gt;
  &lt;li&gt;Unused Code&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If any of factors are less than perfect, you can zoom in and
it provides details on how to improve your code. Like all great
tools, Codacy makes pretty badges to show off your skills.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.codacy.com/public/matthewgiguere/pyutil&quot;&gt;&lt;img src=&quot;https://www.codacy.com/project/badge/24f412c6b6c443f8b6693936594811a1&quot; alt=&quot;Codacy Badge&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;other-badges&quot;&gt;Other Badges&lt;/h3&gt;

&lt;p&gt;I hope you find the above tools useful and implement them in your own
projects. If you are looking for other ways to decorate your
repository README files, you should check out
&lt;a href=&quot;http://shields.io&quot;&gt;shields.io&lt;/a&gt; to find other useful badges, or create
your own.&lt;/p&gt;

</description>
        <pubDate>Fri, 20 Mar 2015 09:20:35 -0400</pubDate>
        <link>http://mattgiguere.github.io/2015/03/20/better-python-coding-practices.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/03/20/better-python-coding-practices.html</guid>
        
        <category>codacy,</category>
        
        <category>coveralls,</category>
        
        <category>travis-ci,</category>
        
        <category>read</category>
        
        <category>the</category>
        
        <category>docs,</category>
        
        <category>python</category>
        
        
      </item>
    
      <item>
        <title>Time Lapse MAO</title>
        <description>&lt;p&gt;The Yale Astronomy Department is putting together a video showcasing some of
the work going on in the department. All of the parts recently arrived for the
Moletai Astronomical Observatory (MAO) Spectrograph that our group is
building. The timing seems perfect, so the videographer in charge setup a
couple cameras to capture the initial build of the spectrograph.&lt;/p&gt;

&lt;p&gt;The photographer sent all of the stills in advance, and we couldn’t wait to
see what the final time lapse would like, so I made a few time lapse videos
myself using &lt;code&gt;ffmpeg&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-ffmpeg&quot;&gt;Setting up ffmpeg&lt;/h3&gt;

&lt;p&gt;To put this together, I needed to install ffmpeg. This turned out to be a \
little challenging. I tried the normal homebrew install:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;brew install ffmpeg&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This worked! I then added a symbolic link to add it to my path:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/local/bin
sudo ln -s /usr/local/Cellar/ffmpeg/2.5.3/bin/ffmpeg ffmpeg&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Unfortunately, when I tried to get help on ffmpeg, here’s what it
returned:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ffmpeg --help
dyld: Library not loaded: /usr/local/lib/libx264.142.dylib
  Referenced from: /usr/local/bin/ffmpeg
  Reason: image not found
Trace/BPT &lt;span class=&quot;nb&quot;&gt;trap&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I then did a search for this .dylib. homebrew installed it:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/local/Cellar
find . -name libx264.142.dylib
./x264/r2495/lib/libx264.142.dylib&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I just needed to add it to my path:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/local/bin
sudo ln -s /usr/local/Cellar/x264/r2495/lib/libx264.142.dylib libx264.142.dylib&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I repeated this for a few more packages homebrew installed that were not in my
path:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;sudo ln -s /usr/local/Cellar/x264/r2495/lib/libx264.142.dylib libx264.142.dylib
sudo ln -s /usr/local/Cellar/libvo-aacenc/0.1.2/lib/libvo-aacenc.0.dylib libvo-aacenc.0.dylib
sudo ln -s /usr/local/Cellar/lame/3.99.5/lib/libmp3lame.0.dylib libmp3lame.0.dylib&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, &lt;code&gt;ffmpeg -h&lt;/code&gt; returned the help, as expected.&lt;/p&gt;

&lt;p&gt;I found &lt;a href=&quot;https://trac.ffmpeg.org/wiki/Create%20a%20video%20slideshow%20from%20images&quot;&gt;this page&lt;/a&gt;
on the ffmpeg wiki to be helpful in creating the time lapse. And
&lt;a href=&quot;https://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion&quot;&gt;Section 7.3&lt;/a&gt; of the documentation too.&lt;/p&gt;

&lt;p&gt;The files I wanted to add were stored in a folder named Bench-View_JPG. They
were in sequential order, but they didn’t start at 0. The easiest way to add
them all to the movie was the use the &lt;code&gt;pattern_type&lt;/code&gt; optional argument in
conjunction with &lt;code&gt;glob&lt;/code&gt;. The other options I used were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;-&lt;em&gt;framerate&lt;/em&gt;: the number of frames per second to input
 -&lt;em&gt;r&lt;/em&gt;: the number of frames per second in the output video&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;s&lt;/em&gt;: the size of the output video&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wanted my video to be 1080p HD, and the final video to be called
&lt;code&gt;BenchMovie1080p11fps.mp4&lt;/code&gt;, so I used the following command:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ffmpeg -framerate &lt;span class=&quot;m&quot;&gt;11&lt;/span&gt; -pattern_type glob -i &lt;span class=&quot;s1&quot;&gt;&amp;#39;Bench-View_JPG/*.jpg&amp;#39;&lt;/span&gt; -r &lt;span class=&quot;m&quot;&gt;11&lt;/span&gt; -s 1620x1080 BenchMovie1080p11fps.mp4&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result can be seen below.&lt;/p&gt;

&lt;video width=&quot;100%&quot; height=&quot;auto&quot; controls=&quot;&quot;&gt;
  &lt;source src=&quot;/images/BenchMovie1080p11fps.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  Note: You are missing out on our time lapse video!
  Upgrade your browser to one that supports the video tag.
&lt;/video&gt;

&lt;p&gt;For embedding the video into the web page, I used the HTML5 &lt;code&gt;&amp;lt;video&amp;gt;&lt;/code&gt; tag. I
also set the width to “100%”” and the height to “auto” to make sure it was
responsive and scaled properly. Here’s the code that made the above video:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;video&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;100%&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;height=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;auto&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;controls&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;source&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/images/BenchMovie1080p11fps.mp4&amp;quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;video/mp4&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
Note: You are missing out on our time lapse video!
Upgrade your browser to one that supports the video tag.
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/video&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The message, of course, only appears in unsupported browsers. Chrome,
Firefox, Internet Explore, Opera, and Safari all support mp4 in the
video tag.&lt;/p&gt;

&lt;p&gt;The result is something that looks great on any browser and any device! I
also created a time lapse video from stills taken with the other camera using
the following command:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ffmpeg -framerate &lt;span class=&quot;m&quot;&gt;11&lt;/span&gt; -pattern_type glob -i &lt;span class=&quot;s1&quot;&gt;&amp;#39;Full-View_JPG/_CAM*.jpg&amp;#39;&lt;/span&gt; -r &lt;span class=&quot;m&quot;&gt;11&lt;/span&gt; -s 1620x1080 FullMovie1080p11fps.mp4&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The results can be seen on MAO page of the &lt;a href=&quot;http://exoplanets.astro.yale.edu/instrumentation/mao.php&quot;&gt;Exoplanets Web Site&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Wed, 18 Mar 2015 08:22:10 -0400</pubDate>
        <link>http://mattgiguere.github.io/2015/03/18/time-lapse-mao.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/03/18/time-lapse-mao.html</guid>
        
        <category>MAO,</category>
        
        <category>ffmpeg,</category>
        
        <category>Yale,</category>
        
        <category>Instrumentation,</category>
        
        <category>mp4,</category>
        
        <category>time</category>
        
        <category>lapse</category>
        
        
      </item>
    
      <item>
        <title>Electric Rates By State</title>
        <description>&lt;p&gt;I was talking with my mother-in-law the other day, and we were wondering what
the highest electric rates in the country are. The US Energy Information
Administration does an excellent job of collecting this information and
displaying it in tabular form on their website. Click &lt;a href=&quot;http://www.eia.gov/electricity/monthly/epm_table_grapher.cfm?t=epmt_5_6_a&quot;&gt;here&lt;/a&gt; to see
what the answer is for the latest month. However, there is enough data in that
table that it took me a minute to skim through, understand every row and
column, and understand the answer to our question. I felt like this data set
deserved a better visualization, so I created it.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://public.tableau.com/javascripts/api/viz_v1.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;tableauPlaceholder&quot; style=&quot;width: 1004px; height: 675px;&quot;&gt;&lt;noscript&gt;&lt;a href=&quot;#&quot;&gt;&lt;img alt=&quot;Dashboard 1 &quot; src=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;el&amp;#47;electricRatesByState201411&amp;#47;Dashboard1&amp;#47;1_rss.png&quot; style=&quot;border: none&quot; /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class=&quot;tableauViz&quot; width=&quot;1004&quot; height=&quot;675&quot; style=&quot;display:none;&quot;&gt;&lt;param name=&quot;host_url&quot; value=&quot;https%3A%2F%2Fpublic.tableau.com%2F&quot; /&gt; &lt;param name=&quot;site_root&quot; value=&quot;&quot; /&gt;&lt;param name=&quot;name&quot; value=&quot;electricRatesByState201411&amp;#47;Dashboard1&quot; /&gt;&lt;param name=&quot;tabs&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;toolbar&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;static_image&quot; value=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;el&amp;#47;electricRatesByState201411&amp;#47;Dashboard1&amp;#47;1.png&quot; /&gt; &lt;param name=&quot;animate_transition&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_static_image&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_spinner&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_overlay&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_count&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;showVizHome&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;showTabs&quot; value=&quot;y&quot; /&gt;&lt;/object&gt;&lt;/div&gt;

&lt;p&gt;I was surprised by both the state with the highest and the state with the
lowest electric rates. Hawai’i has by far the highest average electric rate,
 at 37.6 ¢/kWh, and Washington State has the lowest rate, at only 8.8 ¢/kWh.&lt;/p&gt;

&lt;h3 id=&quot;per-capita-energy-usage&quot;&gt;Per Capita Energy Usage&lt;/h3&gt;

&lt;p&gt;The US EIA also provides a trove of data on many different parameters. I was
then interested to find out who spends the largest fraction of their income
on electric. The US EIA does not provide that information directly, but they
do publish the total retail sales per state.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.census.gov/popest/data/national/totals/2014/index.html&quot;&gt;US Census Bureau publish&lt;/a&gt; another excellent data set tabulating
the estimated population per state per year. I combined these two data sets
using &lt;code&gt;pandas&lt;/code&gt; in Python (see bottom of post for details),
and the resulting average per capita energy use by state can be seen below.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://public.tableau.com/javascripts/api/viz_v1.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;tableauPlaceholder&quot; style=&quot;width: 1004px; height: 675px;&quot;&gt;&lt;noscript&gt;&lt;a href=&quot;#&quot;&gt;&lt;img alt=&quot;DashDesk &quot; src=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;en&amp;#47;energyUsagePerCap201411YTD&amp;#47;DashDesk&amp;#47;1_rss.png&quot; style=&quot;border: none&quot; /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class=&quot;tableauViz&quot; width=&quot;1004&quot; height=&quot;675&quot; style=&quot;display:none;&quot;&gt;&lt;param name=&quot;host_url&quot; value=&quot;https%3A%2F%2Fpublic.tableau.com%2F&quot; /&gt; &lt;param name=&quot;site_root&quot; value=&quot;&quot; /&gt;&lt;param name=&quot;name&quot; value=&quot;energyUsagePerCap201411YTD&amp;#47;DashDesk&quot; /&gt;&lt;param name=&quot;tabs&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;toolbar&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;static_image&quot; value=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;en&amp;#47;energyUsagePerCap201411YTD&amp;#47;DashDesk&amp;#47;1.png&quot; /&gt; &lt;param name=&quot;animate_transition&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_static_image&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_spinner&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_overlay&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_count&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;showVizHome&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;showTabs&quot; value=&quot;y&quot; /&gt;&lt;/object&gt;&lt;/div&gt;

&lt;p&gt;It’s interesting to see that the states with some of
the highest energy prices use the least amount of energy, and some of the
states with the lowest energy prices use the highest amount of energy per
capita.&lt;/p&gt;

&lt;p&gt;I later realized that the US EIA does indeed provide information on
the &lt;a href=&quot;http://www.eia.gov/tools/faqs/faq.cfm?id=97&amp;amp;t=3&quot;&gt;average amount spent per customer by state&lt;/a&gt;, but
not the average amount spent per capita by state. They calculated that
the average Hawaii resident customer uses the least amount of energy and the
average Louisianan customer uses the most energy. This qualitatively agrees
with my findings, but I calculated the result per resident (not per
customer like the EIA).
The average &lt;em&gt;customer&lt;/em&gt; purchases a lot more energy than the average
&lt;em&gt;resident&lt;/em&gt; uses. This makes sense since more than 1 person lives in the
average household.&lt;/p&gt;

&lt;h3 id=&quot;percent-of-income-spent-on-electric&quot;&gt;Percent of Income Spent on Electric&lt;/h3&gt;

&lt;p&gt;The next question I would like to address is what is the fraction of income
that is used towards electricity by state. To answer this I used the results
from the above analysis in addition to the Annual Social and Economic
Supplement Current Income of Households by State Using 2-Year-Average
Medians from the US Census Bureau, which can be downloaded &lt;a href=&quot;http://www.census.gov/hhes/www/income/data/statemedian/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The percent of income spent on electric was calculated by taking the average
per capita energy use by state, multiplying it by the average persons per
household in the US (retrieved from &lt;a href=&quot;http://quickfacts.census.gov/qfd/states/00000.html&quot;&gt;here&lt;/a&gt;), multiplying that by the
average residential electric rate by state for 2014, and dividing by the 2012 -
 2013 2-Year Averaged Median Household income. If you prefer mathematical
notation, that is&lt;/p&gt;

&lt;p&gt;&lt;code&gt;\[
 f_{I} = \frac{E_{C} H_{p} R_{E}}{I}
 \]&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f_{I}&lt;/script&gt;: fraction of income spent on electric&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E_{C}&lt;/script&gt;: Average energy usage per capita by state (&lt;a href=&quot;http://www.eia.gov/electricity/monthly/epm_table_grapher.cfm?t=epmt_5_6_a&quot;&gt;source&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;H_{p}&lt;/script&gt;: The average persons per household in the US (&lt;a href=&quot;http://quickfacts.census.gov/qfd/states/00000.html&quot;&gt;source&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;R_{E}&lt;/script&gt;: The average electric rate per kWh by state (&lt;a href=&quot;http://www.eia.gov/electricity/monthly/epm_table_grapher.cfm?t=epmt_5_6_a&quot;&gt;source&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt;: The median household income by state (&lt;a href=&quot;http://www.census.gov/hhes/www/income/data/statemedian/&quot;&gt;source&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is the result.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://public.tableau.com/javascripts/api/viz_v1.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;tableauPlaceholder&quot; style=&quot;width: 1004px; height: 675px;&quot;&gt;&lt;noscript&gt;&lt;a href=&quot;#&quot;&gt;&lt;img alt=&quot;Dashboard 1 &quot; src=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;el&amp;#47;electricPercentSpent&amp;#47;Dashboard1&amp;#47;1_rss.png&quot; style=&quot;border: none&quot; /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class=&quot;tableauViz&quot; width=&quot;1004&quot; height=&quot;675&quot; style=&quot;display:none;&quot;&gt;&lt;param name=&quot;host_url&quot; value=&quot;https%3A%2F%2Fpublic.tableau.com%2F&quot; /&gt; &lt;param name=&quot;site_root&quot; value=&quot;&quot; /&gt;&lt;param name=&quot;name&quot; value=&quot;electricPercentSpent&amp;#47;Dashboard1&quot; /&gt;&lt;param name=&quot;tabs&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;toolbar&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;static_image&quot; value=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;el&amp;#47;electricPercentSpent&amp;#47;Dashboard1&amp;#47;1.png&quot; /&gt; &lt;param name=&quot;animate_transition&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_static_image&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_spinner&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_overlay&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_count&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;showVizHome&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;showTabs&quot; value=&quot;y&quot; /&gt;&lt;/object&gt;&lt;/div&gt;

&lt;p&gt;This shows that despite having some of the lowest energy rates in the country,
a combination of the lowest median annual income and the highest energy usage
has driven many southern states to spend the highest fraction of their income
on electric. Mississipians spend the most on electric, at 4.5 % of their
income, and Utahans spend the least, at only 1.3 %.&lt;/p&gt;

&lt;h3 id=&quot;appendix&quot;&gt;Appendix&lt;/h3&gt;

&lt;p&gt;All of the code and data used to create this blog post can be found in my
&lt;a href=&quot;https://github.com/mattgiguere/electric&quot;&gt;github repository&lt;/a&gt;
 on energy consumption in the United States. Furthermore, the Tableau
 workbooks that I used to create the above
illustrations can be downloaded for my Tableau public profile &lt;a href=&quot;https://public.tableau.com/profile/mattgiguere#!/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sun, 15 Mar 2015 12:19:59 -0400</pubDate>
        <link>http://mattgiguere.github.io/2015/03/15/electric-rates-by-state.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/03/15/electric-rates-by-state.html</guid>
        
        <category>csvkit,</category>
        
        <category>javascript,</category>
        
        <category>bootstrap,</category>
        
        <category>d3.js</category>
        
        
      </item>
    
      <item>
        <title>Running MPI Python Code on Yale Omega</title>
        <description>&lt;p&gt;Yesterday I got my Parallel Tempering MCMC code working on Yale’s
&lt;a href=&quot;http://westcampus.yale.edu/research/science-medicine-engineering/core-facililties/high-performance-computing-center&quot;&gt;Omega Cluster&lt;/a&gt;. I found &lt;a href=&quot;https://hpc.research.yale.edu/hpc_user_wiki/index.php/Omega&quot;&gt;the “documentation”&lt;/a&gt; to be far
out of date, and not very helpful. Fortunately, there were several people in
the department that could help me out (thanks Kaylea, Duncan, and Andys!!). In
the hopes of easing the reducing the setup time for others that may be
interested in using Omega for their research, I decided to write this blog post
detailing my setup.&lt;/p&gt;

&lt;h3 id=&quot;welcome-email&quot;&gt;Welcome Email&lt;/h3&gt;

&lt;p&gt;When your account is first created, you will receive an email that starts off
like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Welcome to Omega - Yale High Performance Cluster&lt;/p&gt;

  &lt;p&gt;An account has been created for you on omega.hpc.yale.edu.&lt;/p&gt;

  &lt;p&gt;Details about the cluster and its usage can be found at
* http://hpc.research.yale.edu/wiki/index.php/Omega&lt;/p&gt;

  &lt;p&gt;Before you can login you will need to create and upload your ssh key here:
* http://gold.hpc.yale.internal/cgi-bin/sshkeys.py&lt;/p&gt;

  &lt;p&gt;For additional information about ssh please visit:
* http://hpc.yale.edu/faq/secure-shell-faq/&lt;/p&gt;

  &lt;p&gt;There are several queues to choose from, each serving a different purpose
* http://hpc.research.yale.edu/wiki/index.php/Omega#FAS_Queues&lt;/p&gt;

  &lt;p&gt;When submitting jobs, some of the qsub terms have changed; most
importantly when selecting the number of nodes or number of
processors:
* http://hpc.research.yale.edu/wiki/index.php/Omega#Scheduling_your_Programs_to_run&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h4 id=&quot;ssh-keys&quot;&gt;SSH Keys&lt;/h4&gt;
&lt;p&gt;The first thing to note that is not mentioned in the welcome email message (and
I could not find mentioned in the online documentation) is that the link given
to upload your SSH key does not work in the Safari browser. Use Google Chrome.&lt;/p&gt;

&lt;h4 id=&quot;sshing-into-omega&quot;&gt;SSHing Into Omega&lt;/h4&gt;
&lt;p&gt;Next, to SSH into omega, I used the command:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ssh -p22 -i ~/.ssh/myomegakey netid@omega.hpc.yale.edu&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=&quot;creating-a-test-script&quot;&gt;Creating a Test Script&lt;/h4&gt;
&lt;p&gt;Now comes the fun stuff. All jobs submitted for processing need to be wrapped
in a shell script. Before you do anything else, create a test script and see
if it works. The sample script on the HPC site is out of date and results in
error messages. Below is a sample script that works (as of March 12, 2015).&lt;/p&gt;

&lt;p&gt;Contents of &lt;code&gt;my_test_script.sh&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;###fas_devel: for compiling and testing code,&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###restricted to one job per user32 max cores, 4 hours max walltime&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;###name of job&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#PBS -N mytestjob&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;###-q queue_name&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#PBS -q fas_devel&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;###PBS -l procs=1, tpn=1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#PBS -l nodes=1:ppn=8,mem=35gb&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#PBS -l walltime=4:00:00&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;##oe: stdout(o) and stderr(e)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#PBS -j oe&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;##where to put the output&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#PBS -o output_dir/$PBS_JOBNAME.$PBS_JOBID&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;##what you get emails for ((a)borted, (b)egin, (e)nd)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#PBS -m abe&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;###email yourself status messages about your job:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#PBS -M firstname.lastname@yale.edu&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;##Import terminal env variables&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#PBS -V&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OMP_NUM_THREADS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8

&lt;span class=&quot;c&quot;&gt;###run from directory the job is submitted from&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PBS_O_WORKDIR&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;##Command to execute:&lt;/span&gt;
date&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Most of what I wrote above is probably self-explanatory through the comments.
Yes, the ‘#’ signs should be in front of the PBS commands. The &lt;code&gt;$PBS_JOBNAME&lt;/code&gt;
and &lt;code&gt;$PBS_JOBID&lt;/code&gt; are handy variables that can be used to ensure your output is
printed to unique directories (i.e., you’re not overwriting previous results).&lt;/p&gt;

&lt;p&gt;To submit this job, type the following at the command line:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;qsub my_test_script.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should receive an email message when your script has begun execution. You
can see the status of it by typing &lt;code&gt;showq&lt;/code&gt;. The list is usually quite long, so
you might find it useful to pipe the results to grep and search for just lines
that contain your netid:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;showq &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; grep netid&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that this test script is simply printing the date, so it won’t take long
to run, and you should receive a finished email with exit 0 status shortly
after receiving the start email message.&lt;/p&gt;

&lt;p&gt;If all went well, congratulations!&lt;/p&gt;

&lt;h3 id=&quot;setting-up-python-and-mpi-on-omega&quot;&gt;Setting up Python and MPI on Omega&lt;/h3&gt;
&lt;p&gt;Now comes the fun part. Omega uses a module system. To print all available
modules in the Terminal window type the following:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;module avail&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To find specific modules, use the &lt;code&gt;modulefind&lt;/code&gt; command:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;mjg22@login-0-0 ~&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;modulefind python
/home/apps/fas/Modules:
Applications/PythonPackages/numpy/numpy-GCC-ATLAS
Applications/PythonPackages/scipy/scipy-GCC-ATLAS
Applications/PythonPackages/wxPython/2.8
Apps/PythonPackages/numpy/numpy-GCC-ATLAS
Apps/PythonPackages/scipy/scipy-GCC-ATLAS
Apps/PythonPackages/wxPython/2.8
Compilers/Python/2.7.2
Compilers/Python/2.7.3
Compilers/Python/2.7.4rc2_experimental
Compilers/Python/2.7.5
Compilers/Python/2.7.6
Compilers/Python/2.7.9
Compilers/Python/3.2
Compilers/Python/3.3.5
Compilers/Python/OLD-2.7.5
Langs/Python/2.7.2
Langs/Python/2.7.3
Langs/Python/2.7.4rc2_experimental
Langs/Python/2.7.5
Langs/Python/2.7.6
Langs/Python/2.7.9
Langs/Python/3.2
Langs/Python/3.3.5
Langs/Python/OLD-2.7.5
Libraries/IPYTHON/1.1.0
Libraries/NETCDF4-PYTHON/1.1.0
Libraries/WXPYTHON/3.0.0
Libs/IPYTHON/1.1.0
Libs/NETCDF4-PYTHON/1.1.0
Libs/WXPYTHON/3.0.0
RH6/Langs/Python/2.7.2
RH6/Langs/Python/2.7.3
RH6/Langs/Python/2.7.4rc2_experimental
RH6/Langs/Python/3.2
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;mjg22@login-0-0 ~&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As can be seen in the included output above, &lt;code&gt;modulefind&lt;/code&gt; is case insensitive.
There are many versions of python available, but the only packages that I
wanted to uses were numpy and scipy.&lt;/p&gt;

&lt;h4 id=&quot;loading-modules-and-adding-python-packages-using-pip&quot;&gt;Loading Modules and Adding Python Packages Using &lt;code&gt;pip&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Adding more packages to your path can be a little tricky.
I wanted to use python 2.7.9, but when I loaded that module,
the &lt;code&gt;pip&lt;/code&gt; command was not in my path. I loaded several others, and it looks
like the most recent python 2 version to include &lt;code&gt;pip&lt;/code&gt; on Omega is 2.7.3.
Another problem is that you won’t have write access to install packages to
the default site-packages directory, so you will need to create a subdirectory
in your home directory, and specify an optional argument to pip telling it
where to install the libraries you need:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;mjg22@login-0-0 ~&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$mkdir&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;mjg22@login-0-0 ~&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$module&lt;/span&gt; load Langs/Python/2.7.3
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;mjg22@login-0-0 ~&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$module&lt;/span&gt; load Libs/NUMPY/1.9.1
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;mjg22@login-0-0 ~&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$pip&lt;/span&gt; install --install-option&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;--prefix=~/local/&amp;quot;&lt;/span&gt; pandas
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;mjg22@login-0-0 ~&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$pip&lt;/span&gt; install --install-option&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;--prefix=~/local/&amp;quot;&lt;/span&gt; argparse&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;setting-up-your-bashrc-file&quot;&gt;Setting up your .bashrc file&lt;/h3&gt;

&lt;p&gt;Now that we have the python libraries we want to use with our code, we can
modify our bash startup file to load the other modules we want and modify
our python path to include the libraries we installed in ~/local:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;# .bashrc&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Source global definitions&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; -f /etc/bashrc &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt;
        . /etc/bashrc
&lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# User specific aliases and functions&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/fas/fischer/mjg22/local/bin:&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;

module load Langs/Python/2.7.9
module load Libs/NUMPY/1.9.1
module load Libraries/SCIPY/0.14.1
module load MPI/OpenMPI/1.6.5
module load Libs/MPI4PY/1.3.1

&lt;span class=&quot;c&quot;&gt;### Add custom installed python packages to my python path&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### To install these, you need to use Python 2.7.3 because&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### pip does not exist in the more recent modules&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### module load Compilers/Python/2.7.3&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### pip install --install-option=&amp;quot;--prefix=~/local/&amp;quot; package_name&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###for example&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### pip install --install-option=&amp;quot;--prefix=~/local/&amp;quot; argparse&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PYTHONPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/fas/fischer/mjg22/local/lib/python2.7/site-packages:&lt;span class=&quot;nv&quot;&gt;$PYTHONPATH&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;testing-the-real-code&quot;&gt;Testing the real code&lt;/h3&gt;
&lt;p&gt;That should be everything. You should test your code on a single node before
attempting to run it on many cores. &lt;code&gt;fas_devel&lt;/code&gt;, as shown in the test script
above, is the queue you want to use for that. To see what other queues are
available click &lt;a href=&quot;https://hpc.research.yale.edu/hpc_user_wiki/index.php/Omega#FAS_Queues&quot;&gt;here&lt;/a&gt;. Note there are 8 cores per node for
Omega, and 36 GB of RAM per node, as mentioned in the &lt;a href=&quot;https://hpc.research.yale.edu/hpc_user_wiki/index.php/Omega#Hardware&quot;&gt;Hardware&lt;/a&gt;
section.&lt;/p&gt;

&lt;p&gt;I hope you found this post on getting started with MPI and python on the Yale
 Omega cluster helpful!&lt;/p&gt;

</description>
        <pubDate>Thu, 12 Mar 2015 06:06:07 -0400</pubDate>
        <link>http://mattgiguere.github.io/2015/03/12/running-mpi-python-code-on-yale-omega.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/03/12/running-mpi-python-code-on-yale-omega.html</guid>
        
        <category>python,</category>
        
        <category>mpi4py,</category>
        
        <category>mcmc,</category>
        
        <category>omega,</category>
        
        <category>yale,</category>
        
        <category>hpc</category>
        
        
      </item>
    
      <item>
        <title>Creating an AWS EC2 Cluster</title>
        <description>&lt;p&gt;Today I decided to scale up my MPI MCMC sampling to use more nodes.In a
&lt;a href=&quot;/2015/01/27/setting-up-mpi4py.html&quot;&gt;previous blog post&lt;/a&gt;, I showed how I installed MPI on our cluster. I
got it up to 96 cores, but I scaled back and was using about 50 cores from our
cluster for my analysis. I could add more, but there is currently a lot of
demand for CPUs in our research group. Instead,
I’m going try out Amazon Web Services (AWS) Elastic Cloud Computing (EC2) for
this project.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/2015/03/10/scaling-up-with-aws.html&quot;&gt;Yesterday&lt;/a&gt; I setup an AWS account and created an Amazon Machine
Instance (AMI). Today, I’m going to create the cluster. I found
&lt;a href=&quot;http://cs.smith.edu/dftwiki/index.php/Tutorial:_Create_an_MPI_Cluster_on_the_Amazon_Elastic_Cloud_(EC2)&quot;&gt;this post&lt;/a&gt; to be helpful.&lt;/p&gt;

&lt;p&gt;First, I installed &lt;a href=&quot;http://star.mit.edu/cluster/docs/latest/installation.html&quot;&gt;StarCluster&lt;/a&gt;, a python package made
specifically for creating and managing AWS EC2 distributed computing clusters.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;pip install StarCluster&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I then opened a new Terminal window and followed the StarCluster
&lt;a href=&quot;http://star.mit.edu/cluster/docs/latest/quickstart.html&quot;&gt;Quick start&lt;/a&gt; guide. The first thing is to create a config
file:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;∞ starcluster &lt;span class=&quot;nb&quot;&gt;help&lt;/span&gt;
StarCluster - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;http://star.mit.edu/cluster&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;v. 0.95.6&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Software Tools &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; Academics and Researchers &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;STAR&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Please submit bug reports to starcluster@mit.edu

!!! ERROR - config file /home/matt/.starcluster/config does not exist

Options:
--------
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; Show the StarCluster config template
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;2&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; Write config template to /home/matt/.starcluster/config
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;q&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; Quit

Please enter your selection: 2

&amp;gt;&amp;gt;&amp;gt; Config template written to /home/matt/.starcluster/config
&amp;gt;&amp;gt;&amp;gt; Please customize the config template
∞ &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; .starcluster/
∞ emacs config&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Modifying the configuration file to use my AWS_ACCESS_KEY_ID and
AWS_SECRET_ACCESS_KEY was a bit tricky. Amazon changed the way it manages
access keys and no longer allows them to be created at the root level. The
&lt;a href=&quot;http://docs.aws.amazon.com/IAM/latest/UserGuide/Using_SettingUpUser.html&quot;&gt;AWS IAM User documentation&lt;/a&gt; describes how to create a user and
grab their access key. In a nutshell, do this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;go to: https://console.aws.amazon.com/iam/&lt;/li&gt;
  &lt;li&gt;In the left-hand navigation page
    &lt;ul&gt;
      &lt;li&gt;click &lt;strong&gt;Users&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;click the blue &lt;strong&gt;Create New Users&lt;/strong&gt; button at the top&lt;/li&gt;
      &lt;li&gt;enter a username in the field&lt;/li&gt;
      &lt;li&gt;click the blue &lt;strong&gt;Create&lt;/strong&gt; button at the bottom of the screen&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Your new access key and secret keys should then appear on the screen. Copy
these into your .starcluster/config file. For the account number, use your
Account Id, which can be found at the top of &lt;a href=&quot;https://console.aws.amazon.com/billing/home#/account&quot;&gt;“Account Settings”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Scroll down a bit and modify the &lt;code&gt;KEY_LOCATION&lt;/code&gt; to point to the SSH Key you use
to connect to AWS (you had to create a key pair when you created an AWS
  account).&lt;/p&gt;

&lt;p&gt;For me, I also had to modify my &lt;code&gt;AWS_REGION_NAME&lt;/code&gt; and &lt;code&gt;AWS_REGION_HOST&lt;/code&gt;.
Although I’m on the East  coast, for some reason amazon made my instances on
the west coast. After &lt;a href=&quot;https://us-west-2.console.aws.amazon.com/ec2/v2/home?region=us-west-2#Instances:sort=monitoring&quot;&gt;looking at my instances&lt;/a&gt;, here’s what I put
in my config file for those two values:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nv&quot;&gt;AWS_REGION_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; us-west-2a
&lt;span class=&quot;nv&quot;&gt;AWS_REGION_HOST&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; ec2.us-west-2.compute.amazonaws.com&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I attempted to start up my cluster&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;starcluster start mycluster&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After a long pause, I got an error message:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;∞ starcluster start mycluster
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu&lt;/p&gt;

  &lt;p&gt;!!! ERROR - Connection error:
Traceback (most recent call last):
 File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/cli.py”, line 274, in main
   sc.execute(args)
 File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/commands/start.py”, line 189, in execute
   scluster = self.cm.get_cluster_group_or_none(tag)
  File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/cluster.py”, line 244, in &amp;gt;get_cluster_group_or_none
   return self.get_cluster_security_group(group_name)
  File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/cluster.py”, line 240, in &amp;gt;get_cluster_security_group
   return self.ec2.get_security_group(gname)
 File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/awsutils.py”, line 357, in get_security_group
   filters={‘group-name’: groupname})[0]
 File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/awsutils.py”, line 369, in get_security_groups
   return self.conn.get_all_security_groups(filters=filters)
 File “/Applications/anaconda/lib/python2.7/site-packages/boto/ec2/connection.py”, line 2970, in get_all_security_groups
   [(‘item’, SecurityGroup)], verb=’POST’)
 File “/Applications/anaconda/lib/python2.7/site-packages/boto/connection.py”, line 1150, in get_list
   response = self.make_request(action, params, path, verb)
 File “/Applications/anaconda/lib/python2.7/site-packages/boto/connection.py”, line 1096, in make_request
   return self._mexe(http_request)
 File “/Applications/anaconda/lib/python2.7/site-packages/boto/connection.py”, line 926, in _mexe
   request.body, request.headers)
 File “/Applications/anaconda/lib/python2.7/httplib.py”, line 1001, in request
   self._send_request(method, url, body, headers)
 File “/Applications/anaconda/lib/python2.7/httplib.py”, line 1035, in _send_request
   self.endheaders(body)
 File “/Applications/anaconda/lib/python2.7/httplib.py”, line 997, in endheaders
   self._send_output(message_body)
 File “/Applications/anaconda/lib/python2.7/httplib.py”, line 850, in _send_output
   self.send(msg)
 File “/Applications/anaconda/lib/python2.7/httplib.py”, line 812, in send
   self.connect()
 File “/Applications/anaconda/lib/python2.7/site-packages/boto/https_connection.py”, line 116, in connect
   sock = socket.create_connection((self.host, self.port), self.timeout)
 File “/Applications/anaconda/lib/python2.7/socket.py”, line 553, in create_connection
   for res in getaddrinfo(host, port, 0, SOCK_STREAM):
gaierror: [Errno 8] nodename nor servname provided, or not known
!!! ERROR - Check your internet connection?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There were some other options I changed. I’m using the free AMI, a t2.micro, so
I changed the &lt;code&gt;NODE_INSTANCE_TYPE&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nv&quot;&gt;NODE_INSTANCE_TYPE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; t2.micro&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I also setup my instance with the default Amazon Linxu HVM at the top of the
list of instance types. I therefore needed to change the &lt;code&gt;NODE_IMAGE_ID&lt;/code&gt; as
well. This information was given in the comments above that field in the
default ~/.starcluster/config file:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nv&quot;&gt;NODE_IMAGE_ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; ami-6b211202&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This was caused by me changing the &lt;code&gt;AWS_REGION_NAME&lt;/code&gt; and &lt;code&gt;AWS_REGION_HOST&lt;/code&gt;.
Once I changed it back, I could connect, but I got error messages along the
lines of:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;!!! ERROR - UnauthorizedOperation: You are not authorized to perform this operation.
Traceback (most recent call last):
 File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/cli.py”, line 274, in main
   sc.execute(args)
 File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/commands/start.py”, line 189, in execute
   scluster = self.cm.get_cluster_group_or_none(tag)
  File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/cluster.py”, line 244, in &amp;gt;get_cluster_group_or_none
   return self.get_cluster_security_group(group_name)
  File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/cluster.py”, line 240, in &amp;gt;get_cluster_security_group
   return self.ec2.get_security_group(gname)
 File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/awsutils.py”, line 357, in get_security_group
   filters={‘group-name’: groupname})[0]
 File “/Applications/anaconda/lib/python2.7/site-packages/starcluster/awsutils.py”, line 369, in get_security_groups
   return self.conn.get_all_security_groups(filters=filters)
 File “/Applications/anaconda/lib/python2.7/site-packages/boto/ec2/connection.py”, line 2929, in get_all_security_groups
   [(‘item’, SecurityGroup)], verb=’POST’)
 File “/Applications/anaconda/lib/python2.7/site-packages/boto/connection.py”, line 1157, in get_list
   raise self.ResponseError(response.status, response.reason, body)
EC2ResponseError: EC2ResponseError: 403 Forbidden
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;/p&gt;
  &lt;response&gt;&lt;errors&gt;&lt;error&gt;&lt;code&gt;UnauthorizedOperation&lt;/code&gt;&lt;message&gt;You are not authorized to perform this &amp;gt;operation.&lt;/message&gt;&lt;/error&gt;&lt;/errors&gt;&lt;requestid&gt;ccb6c947-8c96-4c41-bd0b-08af6866d50b&lt;/requestid&gt;&lt;/response&gt;
&lt;/blockquote&gt;

&lt;p&gt;To fix this problem, I needed to create a new group that has EC2 permissions,
and add my IAM user to that group. This can be done through the Identity and
Access Management (IAM) &lt;a href=&quot;https://console.aws.amazon.com/iam/home#home&quot;&gt;Dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Click on &lt;strong&gt;Groups&lt;/strong&gt; in the left-hand navigation bar&lt;/li&gt;
  &lt;li&gt;Click on the &lt;strong&gt;Create New Group&lt;/strong&gt; button&lt;/li&gt;
  &lt;li&gt;Add a group with EC2 permissions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once this was done, I started receiving a new error message:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;∞ starcluster start mycluster
StarCluster - (http://star.mit.edu/cluster) (v. 0.95.6)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu&lt;/p&gt;

  &lt;blockquote&gt;
    &lt;blockquote&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Using default cluster template: smallcluster
Validating cluster template settings…
!!! ERROR - Cluster settings are not valid:
!!! ERROR - Keypair ‘myawskey’ does not exist in region ‘us-east-1’&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;At that point I attempted to create a new key using the starcluster
command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;starcluster createkey mysckey -o ~/.ssh/mysckey.rsa&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

</description>
        <pubDate>Wed, 11 Mar 2015 09:58:53 -0400</pubDate>
        <link>http://mattgiguere.github.io/2015/03/11/creating-an-aws-ec2-cluster.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/03/11/creating-an-aws-ec2-cluster.html</guid>
        
        <category>python,</category>
        
        <category>mpi4py,</category>
        
        <category>mcmc,</category>
        
        <category>aws,</category>
        
        <category>amazon,</category>
        
        <category>ec2</category>
        
        
      </item>
    
      <item>
        <title>Scaling Up With AWS</title>
        <description>&lt;p&gt;As mentioned in a &lt;a href=&quot;/2015/03/09/parallel-tempering.html&quot;&gt;previous blog post&lt;/a&gt;, my parallel tempering code is
now working. There is still work to be done in adding new physics,
but I am now in a position to benefit from some extra computing
power. I’m going to try using &lt;a href=&quot;http://aws.amazon.com/ec2/&quot;&gt;Amazon EC2&lt;/a&gt; for this project.&lt;/p&gt;

&lt;p&gt;First, I launched an instance running the Amazon Linux AMI (HVM).
&lt;a href=&quot;http://badhessian.org/2013/11/cluster-computing-for-027hr-using-amazon-ec2-and-ipython-notebook/&quot;&gt;This blog post&lt;/a&gt; may be helpful in setting things up.&lt;/p&gt;

&lt;p&gt;Once the instance was created, and my keys were all setup, I SSHed into my
instance. My favorite distribution of scientific python is anaconda python.
This can be easily downloaded onto the AWS virtual machine instance by&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;going to the Anaconda downloads page&lt;/li&gt;
  &lt;li&gt;selecting the linux OS&lt;/li&gt;
  &lt;li&gt;right-clicking in the browser on the “Linux 64-Bit - Python 2.7” button&lt;/li&gt;
  &lt;li&gt;copying the link to the downloadable package&lt;/li&gt;
  &lt;li&gt;and pasting the link after &lt;code&gt;wget&lt;/code&gt; in the SSH session into the VMI.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ssh -p22 -i my-private-key ec2-user@my-public-ip
mkdir tempDownloads
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;tempDownloads
wget http://09c8d0b2229f813c1b93-c95ac804525aac4b6dba79b00b39d1d3.r79.cf1.rackcdn.com/Anaconda-2.1.0-Linux-x86_64.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once the download has completed, installation is as simple as typing ls to
find out the name of the install script, running it, and following the
on-screen instructions.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ls
-rw-rw-r-- &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; ec2-user ec2-user &lt;span class=&quot;m&quot;&gt;353806962&lt;/span&gt; Sep &lt;span class=&quot;m&quot;&gt;30&lt;/span&gt; 19:37 Anaconda-2.1.0-Linux-x86_64.sh
bash Anaconda-2.1.0-Linux-x86_64.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you said ‘yes’ at the end of installation to have the anaconda python path
appended to your .bashrc file, then starting anaconda python is as simple as
sourcing the changes to your .bashrc file and typing python:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc
python&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There are a few packages I need to run my parallel tempering code. I’ll use
pip to install these:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;pip install emcee&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 10 Mar 2015 14:39:10 -0400</pubDate>
        <link>http://mattgiguere.github.io/2015/03/10/scaling-up-with-aws.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/03/10/scaling-up-with-aws.html</guid>
        
        <category>python,</category>
        
        <category>IPython,</category>
        
        <category>time</category>
        
        <category>series</category>
        
        <category>analysis,</category>
        
        <category>AWS,</category>
        
        <category>EC2</category>
        
        
      </item>
    
      <item>
        <title>Parallel Tempering</title>
        <description>&lt;p&gt;After a few days of working out the bugs in between other projects, I
now have the spot modeling code working with parallel tempering. Briefly,
&lt;a href=&quot;https://en.wikipedia.org/wiki/Parallel_tempering&quot;&gt;parallel tempering&lt;/a&gt; allows for better exploration of the
parameter space of multi-modal distributions. In my particular example, there
may be (and probably are) many spots on the surface of a star. I am trying to
find the most probable coordinates for the spots, as well as their sizes, and
other properties of the star. When performing the normal Affine Invariant MCMC
Ensemble sampling of &lt;a href=&quot;http://dan.iel.fm/emcee/current/&quot;&gt;emcee&lt;/a&gt;, Markov chains will get stuck in local
minima (e.g. smaller spot locations), and do not explore the full parameter
space to find the larger spot locations. Parallel tempering reduces the
height of the barriers between the posterior distribution, allowing chains
to fully explore the parameter space, and find the dominant spot locations.&lt;/p&gt;

&lt;p&gt;Here is the best fit solution employing a maximum likelihood method with an
excellent initial guess based on a grid search:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Show_TwoSpotModelLinearTrendCrat.png&quot; alt=&quot;Maximum Likelihood&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here are 100 random samples taken from the parallel tempering method:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Show_eeTwoSpotParTmp100Samps.png&quot; alt=&quot;Parallel Tempering&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note the standard deviation of the residuals for the photometry is unchanged,
but the standard deviation of the residuals for the RV data benefited from
a 62 cm/s improvement.&lt;/p&gt;

</description>
        <pubDate>Mon, 09 Mar 2015 16:21:08 -0400</pubDate>
        <link>http://mattgiguere.github.io/2015/03/09/parallel-tempering.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/03/09/parallel-tempering.html</guid>
        
        <category>python,</category>
        
        <category>time</category>
        
        <category>series</category>
        
        <category>analysis,</category>
        
        <category>MPI,</category>
        
        <category>Bayesian,</category>
        
        <category>MCMC,</category>
        
        <category>Parallel</category>
        
        <category>Tempering</category>
        
        
      </item>
    
      <item>
        <title>The Last YastroML</title>
        <description>&lt;p&gt;Yesterday we concluded &lt;a href=&quot;https://github.com/YastroML/YastroML&quot;&gt;YastroML&lt;/a&gt;, the Yale astronomy Machine
Learning Discussion Group. Over the past year we have been irregularly meeting
to go over the newly released textbook &lt;a href=&quot;http://press.princeton.edu/titles/10159.html&quot;&gt;Statistics, Data Mining, and Machine
Learning in Astronomy&lt;/a&gt;: a textbook that, as its name implies, goes
over a variety of statistical, data mining, and machine learning techniques
that are relevant to astronomy. As I was looking into ordering the book, I
noticed on twitter that a group of people at Columbia and other institutions
in NYC had started up a discussion group to go over the material, which they
referred to as &lt;a href=&quot;https://github.com/adrn/NYCastroML&quot;&gt;NYCastroML&lt;/a&gt;. This sounded like a terrific idea, and
I immediately decided that I’d try to take part in this weeklyish events. I
started crafting an email to inform other people in the Yale astronomy
Department about the event, and my list of people to include grew, and grew,
and grew… I decided that instead I’d send an email out to the department
to gauge interest in organizing something here at Yale. The response was
overwhelming — &lt;a href=&quot;https://github.com/YastroML/YastroML&quot;&gt;YastroML&lt;/a&gt; was born.&lt;/p&gt;

&lt;p&gt;We started meeting weekly in March of the Spring 2014 semester; we stuck to our
weekly schedule fairly well until the end of the semester. Academics always
seem to think they will have plenty of free time come summer. In practice,
between conferences, workshops, summer schools, and extra curricular
activities, that never seems to be the case. With the end of the Spring
Semester came the end of our weekly schedule. We met sporadically since then,
and our meeting yesterday marked the completion of the final chapter.&lt;/p&gt;

&lt;p&gt;In this last meeting we covered time series analysis. Major topics included&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Discrete Fourier Transforms&lt;/li&gt;
  &lt;li&gt;Periodograms&lt;/li&gt;
  &lt;li&gt;Digital Filtering&lt;/li&gt;
  &lt;li&gt;Wavelets and Matched Filtering&lt;/li&gt;
  &lt;li&gt;Classification of periodic phenomena&lt;/li&gt;
  &lt;li&gt;Analysis of arrival time data&lt;/li&gt;
  &lt;li&gt;Analysis of Stochastic Processes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I put together an &lt;a href=&quot;http://nbviewer.ipython.org/github/YastroML/YastroML/blob/master/Session19_150219/Ch10_TimeSeries.ipynb&quot;&gt;IPython notebook&lt;/a&gt; to lead the discussion. Having
dealt with time series analysis for the past several years while analyzing
RV measurements and light curves, I was comfortable with a lot of the material,
and added several examples and interactive components to the notebook to make
the session a little more pedagogical.&lt;/p&gt;

&lt;p&gt;Some of the highlights for me in this chapter were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;seeing how easy it is to implement wavelets in python using &lt;a href=&quot;http://www.pybytes.com/pywavelets/&quot;&gt;PyWavelets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The use of truncated Fourier Series for searching for signals that may have
multiple reoccurring features, such as eclipsing binaries&lt;/li&gt;
  &lt;li&gt;The nonparametric Gregory-Loredo technique when dealing with sparse data sets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Afterwards a few of us got together for a celebratory dram and to discuss how
the discussion group can be improved for the next time around.&lt;/p&gt;

</description>
        <pubDate>Fri, 20 Feb 2015 12:30:48 -0500</pubDate>
        <link>http://mattgiguere.github.io/2015/02/20/last-yastroml.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/02/20/last-yastroml.html</guid>
        
        <category>python,</category>
        
        <category>IPython</category>
        
        <category>notebook,</category>
        
        <category>time</category>
        
        <category>series</category>
        
        <category>analysis,</category>
        
        <category>YastroML</category>
        
        
      </item>
    
      <item>
        <title>More Spot Modeling</title>
        <description>&lt;p&gt;Continuing from where I &lt;a href=&quot;/2015/02/10/spot-modeling.html&quot;&gt;left off last time&lt;/a&gt;, I tested out a
three spot model today. Here’s the Maximum Likelihood solution
for the two spot model in two dimensions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/TwoSpotModel.png&quot; alt=&quot;Two Spot Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where I used an Aitoff projection to show the spot locations
and relative sizes. And here’s the three spot solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Show_ThreeSpotModel.png&quot; alt=&quot;Three Spot Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the two rightmost spots in the three spot solution are very
similar in size to the rightmost spot in the two spot solution. And
the average latitude of those two rightmost spots in the three spot
solution is approximately the latitude of the single rightmost spot
in the two spot solution.&lt;/p&gt;

&lt;p&gt;Including only three spots and we already see the degenerate
dangers of spot modeling. On the two do list is to take a
bayesian approach and use the &lt;code&gt;emcee&lt;/code&gt; MCMC sampler to further
explore parameter space and make sure the two rightmost spots in
the two spot solution are not both getting caught in the same
local minimum.&lt;/p&gt;

&lt;p&gt;We also have a lot of prior information we know for this problem
that we’ll be able to include when taking the Bayesian approach.
More on that later…&lt;/p&gt;

</description>
        <pubDate>Thu, 12 Feb 2015 15:45:45 -0500</pubDate>
        <link>http://mattgiguere.github.io/2015/02/12/more-spot-modeling.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/02/12/more-spot-modeling.html</guid>
        
        <category>python,</category>
        
        <category>simulation,</category>
        
        <category>animation,</category>
        
        <category>pandas,</category>
        
        <category>map</category>
        
        <category>projections,</category>
        
        <category>aitoff</category>
        
        
      </item>
    
      <item>
        <title>Stellar Spot Modeling</title>
        <description>&lt;p&gt;Over the past two days I worked on fitting the simultaneous MOST photometry
and CHIRON RV measurements of Epsilon Eridani. This document describes
some of the more general code as well as the early results. The full
spot modeling code will be made publicly available when the peer-reviewed
paper is accepted.&lt;/p&gt;

&lt;h4 id=&quot;importing-the-data&quot;&gt;Importing the Data&lt;/h4&gt;

&lt;p&gt;After finding out over the weekend that the times we stored in IDL data
structures (and published over the last 6 years) were erroneous, I used
the exposure meter midpoints from the MySQL database instead:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;SELECT emmnwob, mnvel, errvel FROM exposuremeter e &amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;INNER JOIN velocities v ON v.observation_id = e.observation_id &amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;INNER JOIN observations o ON o.observation_id = e.observation_id &amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;WHERE object=22049 AND tag=&amp;#39;a&amp;#39; AND mnvel IS NOT NULL&amp;quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ccdb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connectChironDB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;eerv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_sql_query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And converted the exposure meter midpoints to Julian Dates using
the &lt;code&gt;pandas&lt;/code&gt; &lt;code&gt;DatetimeIndex&lt;/code&gt; and &lt;code&gt;to_julian_date()&lt;/code&gt; methods:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;emmnwobjds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DatetimeIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eerv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emmnwob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eerv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;emmnwobjd&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emmnwobjds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_julian_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next step was to restore the MOST data from file and downsample by taking a
running mean. Downsampling reduced the noise and allowed the photometry to
be combined with the RV data without completely dominating the solution.
Both restoring the data and downsampling were accomplished in
pandas with a little help from the &lt;code&gt;Time&lt;/code&gt; method in &lt;code&gt;astropy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;astropy.time&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#restore data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eeph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;epsEriMost.txt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;\s+&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skipinitialspace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#create JD astropy Time object for easy conversion to ISO format:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;MostJds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eeph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MJD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2451545.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;jd&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#now store as DatetimeIndex&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eeph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;ObsDT&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DatetimeIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MostJds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#next create a one column series of flux values with the observation&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#time as the index:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eemr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eeph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;flux&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eeph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;ObsDT&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mhc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;Mean&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;downsampledtimes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eemr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;480min&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;how&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mhc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_julian_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;downsampledflux&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eemr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;480min&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;how&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mhc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since the MOST data have reported homoscedastic errors (a rarity in astronomy)
this worked out. Otherwise, we’d need to take a weighted mean.&lt;/p&gt;

&lt;h4 id=&quot;the-maximum-likelihood-method&quot;&gt;The Maximum Likelihood Method&lt;/h4&gt;

&lt;p&gt;We would like to fit the spectroscopic and photometric data simultaneously
to get a self-consistent model describing the star. For a first step in doing
this, I went with a quick maximum likelihood approach. The log likehood function
for the problem is:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;\[
\ln{p(v, \phi|t, \sigma, t&#39;, \sigma&#39;, \Theta)} = - \frac{1}{2} \sum_{n}\left[ \frac{(v_{n} - f(t_{n}| \Theta))^{2}}{\sigma_{n}^2} + \ln{(2 \pi \sigma_{n}^{2})}\right]
- \frac{1}{2} \sum_{m}\left[ \frac{(\phi_{m} - f&#39;(t&#39;_{m}| \Theta))^{2}}{\sigma_{m}^{&#39;2}} + \ln{(2 \pi \sigma_{m}^{&#39;2})}\right]
\]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;v_{n}&lt;/script&gt; is the radial velocity measurement for the &lt;script type=&quot;math/tex&quot;&gt;n^{th}&lt;/script&gt; observation taken at time &lt;script type=&quot;math/tex&quot;&gt;t_{n}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\sigma_{n}&lt;/script&gt; is the single measurement uncertainty, and &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is the radial velocity spot model given our hyperparamers, &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\phi_{m}&lt;/script&gt; is the flux of the &lt;script type=&quot;math/tex&quot;&gt;m^{th}&lt;/script&gt; photometric observation happening at time &lt;script type=&quot;math/tex&quot;&gt;t&#39;&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f&#39;&lt;/script&gt; is the photometric spot model given our same hyperparameters, &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\sigma_{m}^{&#39;}&lt;/script&gt; is the photometric uncertainty of the &lt;script type=&quot;math/tex&quot;&gt;m^{th}&lt;/script&gt; observation. The hyperparameters here are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the latitude of spot 1&lt;/li&gt;
  &lt;li&gt;the phase of spot 1&lt;/li&gt;
  &lt;li&gt;the fractional area of spot 1&lt;/li&gt;
  &lt;li&gt;the latitude of spot 2&lt;/li&gt;
  &lt;li&gt;the phase of spot 2&lt;/li&gt;
  &lt;li&gt;the fractional area of spot 2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was then driven by minimizing the negative of the log likelihood:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#use the scipy optimize method:&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.optimize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;op&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nll&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lnlike&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ferr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;Powell&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For a first pass we decided to include two spots. No spot growth or decay.
No differential rotation. Just something quick and dirty, but it was clear
at least two spots were needed. Below is an animation showing the result.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Show_ObsAnim12medPause.gif&quot; alt=&quot;Spot Modeling Photometry and Spectroscopy Simultaneously&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Feb 2015 13:35:53 -0500</pubDate>
        <link>http://mattgiguere.github.io/2015/02/10/spot-modeling.html</link>
        <guid isPermaLink="true">http://mattgiguere.github.io/2015/02/10/spot-modeling.html</guid>
        
        <category>python,</category>
        
        <category>imagemagick,</category>
        
        <category>simulation,</category>
        
        <category>animation,</category>
        
        <category>pandas</category>
        
        
      </item>
    
  </channel>
</rss>
